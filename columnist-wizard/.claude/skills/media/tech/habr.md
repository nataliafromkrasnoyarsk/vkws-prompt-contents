---
name: habr
description: Style guide and examples for writing expert articles in Habr (Хабр) corporate blogs. Use when creating technical content for habr.com.
---

# Habr / Хабр — Руководство по стилю

## Профиль издания

| Параметр | Значение |
|----------|----------|
| URL | habr.com |
| Аудитория | Разработчики, архитекторы, DevOps, tech leads |
| Объём статьи | 1,500-3,000 слов (8,000+ символов) |
| Технический уровень | 60-80% |
| Тон | Технический, аутентичный, без маркетинга |
| Формат | Корпоративные блоги, Markdown |

## Критические правила Хабра

### КАТЕГОРИЧЕСКИ ЗАПРЕЩЕНО
- Маркетинговая риторика («лучшее решение», «уникальный продукт»)
- Пресс-релизный стиль
- Поверхностное изложение без глубины
- Реклама продуктов/услуг
- Отсутствие практической ценности

### ОБЯЗАТЕЛЬНО
- Техническая глубина и детали
- Код, примеры, конфиги
- Честность об ограничениях и проблемах
- Готовность отвечать на комментарии
- Практическая применимость

### Реакция сообщества
- Маркетинг → минусы, негативные комментарии
- Поверхностность → «Капитан Очевидность»
- Хорошая статья → 100+ комментариев, дискуссия

## Структура статьи

```
ЗАГОЛОВОК
├── Конкретный и технический
├── Может содержать результат («...и сэкономили 40% CPU»)
└── Без кликбейта

ПРЕВЬЮ (до <cut/>)
├── 500-1,500 символов
├── Суть проблемы и решения
├── Зачем читать дальше
└── <cut/> — обязательный тег

ВВЕДЕНИЕ
├── Контекст и проблема
├── Почему стандартные решения не подошли
└── Что мы сделали (спойлер результата)

ТЕЛО (3-7 секций)
├── Техническая часть с кодом
├── Схемы и диаграммы
├── Метрики и бенчмарки
├── Проблемы, которые встретили
└── Как решали

РЕЗУЛЬТАТЫ
├── Конкретные цифры
├── Сравнение до/после
└── Что можно улучшить

ВЫВОДЫ
├── Ключевые takeaways
├── Когда применять / не применять
└── Ссылки на репозитории, документацию
```

## Форматирование

### Markdown-элементы
```markdown
# H1 — только заголовок статьи
## H2 — основные секции
### H3 — подсекции

**жирный** для важных терминов
`inline code` для команд и переменных
```code block``` для примеров кода

> Цитаты для важных замечаний

- Списки для перечислений
1. Нумерованные для последовательностей

| Таблицы | для | сравнений |
```

### Код
- Указывать язык для подсветки: ```python
- Комментарии в коде на русском или английском
- Рабочие примеры, не псевдокод
- Ссылка на репозиторий, если есть

### Типографика
- Тире: — (не -)
- Кавычки: «ёлочки» или "лапки"
- Буква ё: использовать
- Неразрывные пробелы перед %, №, единицами

## Тональность

### Как писать
- Как рассказываешь коллеге за кофе
- Честно о проблемах и факапах
- Признавать, что не знаешь
- Использовать «мы» или «я»
- Допустим лёгкий юмор

### Как НЕ писать
- Как пресс-релиз
- Как рекламный буклет
- Как академическая статья
- Снисходительно к читателям

## Формат биографии

В корпоративном блоге биография — в профиле компании.
В конце статьи можно добавить:

```
---
Если есть вопросы — пишите в комментариях, постараюсь ответить.
[Ссылка на наш блог] | [Telegram-канал]
```

---

## Примеры статей

### Пример 1: Техническая статья — VK Private Cloud

**Заголовок:** Препарируем VK Private Cloud: подробнейшие детали из первых уст

**Превью:**
У платформы VK Cloud есть продукт, который позволяет компаниям частично или полностью перенести свою инфраструктуру не в публичное, а в частное облако. Хранить всё в своём ЦОД и под личным контролем — но пользоваться интерфейсом и инструментами, разработанными VK Tech. Рассказываем, как это устроено под капотом.

<cut/>

**Текст:**

## Предыстория: от публичного облака к Private Cloud

VK Private Cloud — продукт, который позволяет развернуть облачную платформу в собственном дата-центре клиента. Это не просто «установка OpenStack» — это полноценная платформа со всеми сервисами VK Cloud, которую мы разворачиваем на чистые серверы десятки раз за рабочий день.

Раньше новые регионы запускались около года. Теперь это вопрос пары месяцев. Расскажем, как мы пришли к такой скорости.

К середине 2023 года мы имели:
- Кластер на 800+ нод
- Время деплоя некоторых сервисов выросло с 2 до 15 минут
- Control plane начал показывать признаки stress'а
- Один неудачный деплой мог теоретически положить всю платформу

## Первый звоночек: scheduler и его limits

Первые проблемы начались с того, что scheduler стал заметно медленнее принимать решения. При 800+ нодах и тысячах подов расчёт оптимального размещения превратился в нетривиальную задачу.

```yaml
# Типичный манифест, который scheduler обрабатывал 30+ секунд
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-service
spec:
  replicas: 50
  selector:
    matchLabels:
      app: critical-service
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - critical-service
              topologyKey: kubernetes.io/hostname
      containers:
        - name: app
          resources:
            requests:
              cpu: "500m"
              memory: "512Mi"
            limits:
              cpu: "1000m"
              memory: "1Gi"
```

Мы пробовали разные подходы:

```bash
# Увеличение QPS для API server
--kube-api-qps=100
--kube-api-burst=200

# Результат: помогло частично, проблема вернулась через 3 месяца
```

## Путь к multi-cluster

После инцидента (4 часа даунтайма из-за бага в операторе) мы приняли решение о переходе к multi-cluster архитектуре.

### Определение границ кластеров

```
┌─────────────────────────────────────────────────────┐
│                   КРИТИЧНЫЕ                        │
│  Billing, Auth, Core API                           │
│  SLA: 99.99%, отдельный кластер                    │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│                   ПРОДУКТОВЫЕ                      │
│  S3, Compute, Managed K8s, Database                │
│  SLA: 99.95%, кластер на продукт                   │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│                   ВСПОМОГАТЕЛЬНЫЕ                  │
│  CI/CD runners, staging, tools                     │
│  SLA: 99.9%, общий кластер                         │
└─────────────────────────────────────────────────────┘
```

### GitOps с ArgoCD

```yaml
# ApplicationSet для управления всеми кластерами
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: cluster-apps
  namespace: argocd
spec:
  generators:
    - git:
        repoURL: https://github.com/our-org/k8s-configs
        revision: HEAD
        directories:
          - path: clusters/*
  template:
    metadata:
      name: '{{path.basename}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/our-org/k8s-configs
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: '{{path.basename}}.cluster.local'
        namespace: default
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

## Результаты

| Метрика | Было (монолит) | Стало (multi-cluster) |
|---------|----------------|----------------------|
| Время деплоя | 5-15 мин | 1-3 мин |
| Blast radius | Вся платформа | Один продукт |
| Время восстановления | 2-4 часа | 15-30 мин |
| Накладные control plane | ~5% | ~8% |

## Грабли, на которые наступили

**1. DNS между кластерами**
Cross-cluster DNS — боль. В итоге ExternalDNS + Route53, но путь был тернистым.

**2. Secrets management**
Один Vault на все кластеры + разные service accounts = много YAML и много возможностей ошибиться.

**3. Capacity planning**
Теперь планируем отдельно для каждого кластера. Cluster Autoscaler помогает, но не решает проблему полностью.

## Когда multi-cluster НЕ нужен

- Меньше 100 нод
- Нет проблем с performance
- Сервисы сильно связаны
- Нет команды для поддержки

Монолитный кластер — не антипаттерн. Просто не для нашего масштаба.

## Выводы

1. При определённом масштабе накладные расходы монолита превышают multi-cluster
2. Blast radius — ключевой критерий архитектурных решений
3. GitOps обязателен для управления несколькими кластерами
4. Миграция — марафон, не спринт

---

Буду рад ответить на вопросы в комментариях.

**Оценка:** Отличный пример для Хабра. Техническая глубина, реальный код, честность о проблемах, практическая ценность.

---

### Пример 2: Статья-исследование

**Заголовок:** Сравниваем 5 способов деплоя в Kubernetes: какой выбрать в 2025

**Превью:**
Rolling update, Blue-Green, Canary, A/B, Progressive Delivery — разбираемся, когда какой использовать. С примерами манифестов, метриками и граблями из продакшена.

<cut/>

**Текст:**

## Зачем эта статья

В нашей команде новички регулярно спрашивают: «А почему мы используем canary, а не blue-green?» Вместо того чтобы объяснять каждый раз, решил собрать всё в одном месте.

Статья для тех, кто уже работает с Kubernetes, но хочет систематизировать знания о стратегиях деплоя.

## Критерии сравнения

Буду оценивать по 5 параметрам:
- **Сложность настройки** (1-5, где 5 — сложно)
- **Потребление ресурсов** (сколько дополнительных pod'ов)
- **Скорость отката** (время до полного rollback)
- **Гранулярность** (насколько точно можно контролировать трафик)
- **Требования к инфраструктуре** (что нужно дополнительно)

## 1. Rolling Update

Встроенная стратегия Kubernetes. Постепенно заменяет старые pod'ы новыми.

```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%        # сколько pod'ов можно добавить сверх desired
      maxUnavailable: 25%  # сколько pod'ов может быть недоступно
```

**Плюсы:**
- Работает из коробки
- Минимальное потребление ресурсов
- Простая настройка

**Минусы:**
- Нет контроля трафика (старые и новые pod'ы получают трафик одновременно)
- Откат через `kubectl rollout undo` — не мгновенный
- Сложно тестировать на проде

**Когда использовать:** Для некритичных сервисов, где допустим смешанный трафик.

| Критерий | Оценка |
|----------|--------|
| Сложность | 1/5 |
| Ресурсы | +25% во время деплоя |
| Скорость отката | 1-5 мин |
| Гранулярность | Нет |
| Требования | Нет |

## 2. Blue-Green Deployment

Две идентичные среды: blue (текущая) и green (новая). Переключение трафика одним действием.

```yaml
# Service, который переключается между версиями
apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  selector:
    app: my-app
    version: green  # меняем на blue для отката
  ports:
    - port: 80
```

**Плюсы:**
- Мгновенное переключение
- Мгновенный откат
- Чистое разделение версий

**Минусы:**
- Требует 2x ресурсов
- Нет постепенного тестирования
- Сложнее с stateful-приложениями

**Когда использовать:** Для критичных сервисов, где важен мгновенный откат.

| Критерий | Оценка |
|----------|--------|
| Сложность | 2/5 |
| Ресурсы | +100% постоянно |
| Скорость отката | Секунды |
| Гранулярность | Нет (всё или ничего) |
| Требования | Нет |

## 3. Canary Deployment

Новая версия получает небольшой процент трафика. Постепенно увеличиваем при успехе.

```yaml
# С Istio VirtualService
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-app
spec:
  hosts:
    - my-app
  http:
    - route:
        - destination:
            host: my-app
            subset: stable
          weight: 90
        - destination:
            host: my-app
            subset: canary
          weight: 10
```

**Плюсы:**
- Минимальный риск (только 10% трафика на новую версию)
- Возможность A/B тестирования
- Постепенное увеличение нагрузки

**Минусы:**
- Требует service mesh или ingress controller
- Сложнее настройка
- Нужен мониторинг для автоматизации

**Когда использовать:** Для высоконагруженных сервисов, где важно протестировать на реальном трафике.

| Критерий | Оценка |
|----------|--------|
| Сложность | 3/5 |
| Ресурсы | +10-50% во время деплоя |
| Скорость отката | Секунды (0% на canary) |
| Гранулярность | Высокая (любой %) |
| Требования | Service mesh / Ingress |

[... продолжение с A/B и Progressive Delivery ...]

## Итоговая таблица

| Стратегия | Сложность | Ресурсы | Откат | Гранулярность | Для чего |
|-----------|-----------|---------|-------|---------------|----------|
| Rolling | ⭐ | +25% | Минуты | Нет | Некритичные сервисы |
| Blue-Green | ⭐⭐ | +100% | Секунды | Нет | Критичные, stateless |
| Canary | ⭐⭐⭐ | +10-50% | Секунды | Высокая | Высоконагруженные |
| A/B | ⭐⭐⭐⭐ | +50-100% | Секунды | По сегментам | Продуктовые эксперименты |
| Progressive | ⭐⭐⭐⭐⭐ | +10-50% | Авто | Автоматическая | SRE-зрелые команды |

## Что используем мы

- **Core API**: Blue-Green (критичность важнее ресурсов)
- **Web-приложения**: Canary с Istio
- **Внутренние сервисы**: Rolling Update
- **ML-пайплайны**: Progressive Delivery с Argo Rollouts

---

Если есть вопросы или хотите поделиться своим опытом — пишите в комментариях.

**Оценка:** Хороший пример обзорной статьи. Структурированное сравнение, практические примеры, честные рекомендации.

---

## Чеклист перед публикацией

- [ ] Объём 1500-3000 слов
- [ ] Технический заголовок без кликбейта
- [ ] Превью 500-1500 символов до `<cut/>`
- [ ] Код с подсветкой синтаксиса
- [ ] Схемы/диаграммы где нужно
- [ ] Конкретные метрики и цифры
- [ ] Честность о проблемах и ограничениях
- [ ] Практическая применимость
- [ ] Нет маркетинговой риторики
- [ ] Правильная типографика (—, «», ё)
- [ ] Готовность отвечать на комментарии
