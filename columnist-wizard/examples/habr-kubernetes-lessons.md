# Пример колонки для Habr

## Метаданные
- **Издание**: Habr (корпоративный блог)
- **Тип**: Из опыта (Experience-Based)
- **Объём**: ~2000 слов
- **Автор**: Алексей Петров, Lead SRE в VK Cloud

---

# Почему мы отказались от монолитного Kubernetes и не жалеем: история одной миграции

Три года назад мы были уверены, что один большой кластер Kubernetes решит все наши проблемы. Спойлер: не решил. Рассказываю, как мы пришли к multi-cluster архитектуре и какие грабли собрали по дороге.

<cut/>

## Предыстория: «один кластер, чтобы править всеми»

В 2021 году наша инфраструктура состояла из ~200 микросервисов, обслуживающих несколько продуктов VK Cloud. Логика была простой: один кластер = единая точка управления, общие ресурсы, меньше накладных расходов на администрирование.

На бумаге выглядело красиво. В продакшене — не очень.

К середине 2023 года мы имели:
- Кластер на 800+ нод
- Время деплоя некоторых сервисов выросло с 2 до 15 минут
- Control plane начал показывать признаки stress'а
- Один неудачный деплой мог теоретически положить всю платформу

Звучит как типичная история роста, но дьявол, как всегда, в деталях.

## Первый звоночек: scheduler и его limits

Первые проблемы начались с того, что scheduler стал заметно медленнее принимать решения. При 800+ нодах и тысячах подов расчёт оптимального размещения превратился в нетривиальную задачу.

```yaml
# Было: простой запрос
resources:
  requests:
    cpu: "100m"
    memory: "128Mi"

# Стало: scheduler думает 30+ секунд
# при высокой утилизации кластера
```

Мы пробовали разные подходы:
- Увеличение ресурсов control plane (помогло частично)
- Настройка `--kube-api-qps` и `--kube-api-burst` (помогло, но ненадолго)
- Priority classes для критичных workloads (обязательно, но не решает корень проблемы)

В какой-то момент стало очевидно: мы лечим симптомы, а не болезнь.

## Второй звоночек: blast radius

Инцидент февраля 2023 года стал переломным. Из-за бага в одном из операторов (не будем показывать пальцем, но история была связана с CRD) кластер ушёл в нестабильное состояние. На восстановление ушло 4 часа. Четыре часа даунтайма для всех продуктов.

После разбора полётов команда задала правильный вопрос: «А почему отказ одного компонента влияет на все остальные?»

Ответ был неприятным: потому что мы сами так спроектировали.

## Путь к multi-cluster: не так просто, как кажется

Решение о переходе к multi-cluster архитектуре далось непросто. Основные опасения:
1. Рост операционной сложности (N кластеров = N точек отказа?)
2. Увеличение накладных расходов на control plane
3. Сложности с service mesh и cross-cluster communication
4. Неизбежный период нестабильности при миграции

Но аргументы «за» перевесили:
1. Изоляция blast radius (критичный фактор)
2. Возможность обновлять кластеры независимо
3. Гибкость в выборе версий K8s для разных workloads
4. Лучшая утилизация через right-sizing кластеров

## Как мы это сделали: пошаговый план

### Шаг 1: Определение границ кластеров

Мы не стали дробить по принципу «один сервис = один кластер». Слишком дорого и сложно. Вместо этого выделили три категории:

```
┌─────────────────────────────────────────────────────┐
│                   КРИТИЧНЫЕ                        │
│  Billing, Auth, Core API                           │
│  Требования: 99.99%, минимальный blast radius      │
│  → Отдельный кластер с усиленным SLA               │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│                   ПРОДУКТОВЫЕ                      │
│  S3, Compute, Managed K8s, Database                │
│  Требования: 99.95%, изоляция между продуктами     │
│  → Кластер на продукт или группу продуктов         │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│                   ВСПОМОГАТЕЛЬНЫЕ                  │
│  CI/CD runners, staging, internal tools            │
│  Требования: 99.9%, допустимы перерывы             │
│  → Общий кластер, можно экономить                  │
└─────────────────────────────────────────────────────┘
```

### Шаг 2: Service Mesh или не Service Mesh?

Долго дискутировали про Istio для cross-cluster коммуникации. В итоге отказались по нескольким причинам:

1. Дополнительный latency на каждый hop
2. Сложность отладки при проблемах
3. Большинство наших сервисов общаются через Kafka и PostgreSQL, а не напрямую

Вместо этого использовали:
- Внешний балансировщик для public-facing сервисов
- Kafka как шину событий между кластерами
- Прямые подключения к managed-сервисам (PostgreSQL, Redis) вне K8s

### Шаг 3: GitOps и multi-cluster management

Для управления несколькими кластерами внедрили ArgoCD с App of Apps pattern:

```yaml
# Структура репозитория
clusters/
├── critical/
│   ├── billing/
│   ├── auth/
│   └── core-api/
├── products/
│   ├── s3/
│   ├── compute/
│   └── managed-k8s/
└── shared/
    ├── ci-runners/
    └── internal-tools/

# ArgoCD ApplicationSet
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: cluster-apps
spec:
  generators:
    - git:
        repoURL: https://github.com/our-org/k8s-configs
        directories:
          - path: clusters/*
```

Это дало нам единую точку правды при сохранении изоляции.

### Шаг 4: Observability

Самый недооценённый аспект multi-cluster. У нас теперь N источников метрик, логов и трейсов. Решили так:

- **Метрики**: Thanos для federation Prometheus'ов
- **Логи**: Loki с multi-tenant mode
- **Трейсы**: Jaeger с централизованным collector'ом

Ключевой инсайт: **cluster_name** как обязательный label для всего. Без этого debugging превращается в ад.

## Результаты: что получилось

Спустя год работы в multi-cluster режиме:

| Метрика | Было (монолит) | Стало (multi-cluster) |
|---------|----------------|----------------------|
| Время деплоя | 5-15 мин | 1-3 мин |
| Blast radius инцидента | Вся платформа | Один продукт |
| Время восстановления | 2-4 часа | 15-30 мин |
| Накладные расходы на control plane | ~5% | ~8% |

Да, мы платим больше за инфраструктуру control plane. Но экономия на инцидентах и времени команды с лихвой компенсирует эти затраты.

## Грабли, на которые мы наступили

Честно про то, что пошло не так:

### 1. Недооценили DNS

Cross-cluster DNS — это боль. Kubernetes DNS отлично работает внутри кластера, но между кластерами начинается творческая инженерия. Мы в итоге использовали ExternalDNS + Route53, но путь был тернистым.

### 2. Secrets management стал сложнее

Один Vault на все кластеры + разные service accounts + разные namespaces = много YAML и много возможностей ошибиться. Автоматизируйте это с первого дня.

### 3. Capaciy planning для каждого кластера

Раньше у нас был один большой пул ресурсов. Теперь нужно планировать capacity отдельно для каждого кластера. Cluster Autoscaler помогает, но не решает проблему полностью.

### 4. Тестовые окружения

Staging в multi-cluster мире — отдельная песня. Мы в итоге сделали «mini» версии продуктовых кластеров для staging, но это удвоило количество сущностей, которые нужно поддерживать.

## Когда multi-cluster НЕ нужен

Важный disclaimer: multi-cluster — не серебряная пуля. Не переходите на эту архитектуру, если:

- У вас меньше 100 нод и нет проблем с performance
- Ваши сервисы сильно связаны и требуют низкого latency между собой
- Нет команды для поддержки сложной инфраструктуры
- Бизнес не готов к временному увеличению расходов

Монолитный кластер — не антипаттерн. Это разумный выбор для многих случаев. Просто не для нашего масштаба.

## Выводы

1. **Размер имеет значение**. При определённом масштабе накладные расходы монолитного кластера начинают превышать накладные расходы multi-cluster.

2. **Blast radius — ключевой критерий**. Если отказ одного компонента может положить всю платформу, это архитектурная проблема, а не операционная.

3. **GitOps обязателен**. Управлять несколькими кластерами руками — путь к безумию.

4. **Observability важнее, чем кажется**. Инвестируйте в централизованный мониторинг с первого дня.

5. **Миграция — марафон, не спринт**. Мы переходили постепенно, один продукт за другим. Попытка «big bang» миграции закончилась бы катастрофой.

Если вы сейчас на развилке и думаете о multi-cluster — надеюсь, наш опыт поможет принять решение. Или хотя бы собрать меньше граблей, чем мы.

---

Буду рад ответить на вопросы в комментариях. Особенно интересно услышать, как вы решаете подобные проблемы — может, мы что-то упустили.
